# Azure Workflow Analysis: Rollback and Destroy

**Analysis Date:** November 24, 2025  
**Workflows Analyzed:**
- `deploy-azure-infrastructure.yml` - Deployment with automatic rollback on failure
- `destroy-azure-infrastructure.yml` - Manual infrastructure teardown

---

## Executive Summary

‚úÖ **Rollback Workflow:** Properly destroys partially created infrastructure on deployment failure  
‚úÖ **Destroy Workflow:** Properly destroys all infrastructure resources  
‚ö†Ô∏è **Critical Gap:** Key Vault not explicitly targeted in destroy operations  
‚úÖ **OIDC Preservation:** Both workflows correctly preserve OIDC configuration for reuse

---

## 1. Complete Azure Resource Inventory

### Resources Created by Terraform

#### Main Resources (main.tf)
```
1. azurerm_resource_group.main
   ‚îî‚îÄ Resource Group: testcontainers-{env}-rg
```

#### Module: OIDC (modules/oidc/)
```
2. azuread_application.github_actions
   ‚îî‚îÄ Azure AD App Registration
3. azuread_application_federated_identity_credential.github_main
   ‚îî‚îÄ OIDC Federated Credential (main branch)
4. azuread_application_federated_identity_credential.github_pr
   ‚îî‚îÄ OIDC Federated Credential (pull requests)
5. azuread_application_federated_identity_credential.github_environment
   ‚îî‚îÄ OIDC Federated Credential (environments)
6. azuread_service_principal.github_actions
   ‚îî‚îÄ Service Principal
7. azurerm_role_assignment.github_actions_contributor
   ‚îî‚îÄ Contributor Role Assignment
8. azurerm_role_assignment.github_actions_user_access_admin
   ‚îî‚îÄ User Access Administrator Role Assignment
```

#### Module: Networking (modules/networking/)
```
9. azurerm_virtual_network.main
   ‚îî‚îÄ VNet: testcontainers-{env}-vnet
10. azurerm_subnet.public
    ‚îî‚îÄ Public Subnet
11. azurerm_subnet.private
    ‚îî‚îÄ Private Subnet
12. azurerm_public_ip.nat
    ‚îî‚îÄ NAT Gateway Public IP
13. azurerm_nat_gateway.main
    ‚îî‚îÄ NAT Gateway
14. azurerm_nat_gateway_public_ip_association.main
    ‚îî‚îÄ NAT Gateway IP Association
15. azurerm_subnet_nat_gateway_association.main
    ‚îî‚îÄ Subnet NAT Gateway Association
```

#### Module: Security (modules/security/)
```
16. azurerm_network_security_group.main
    ‚îî‚îÄ NSG: testcontainers-{env}-nsg
17. azurerm_network_security_rule.allow_http
    ‚îî‚îÄ NSG Rule: allow-http
18. azurerm_network_security_rule.allow_https
    ‚îî‚îÄ NSG Rule: allow-https
19. azurerm_network_security_rule.allow_outbound
    ‚îî‚îÄ NSG Rule: allow-outbound
20. azurerm_subnet_network_security_group_association.private
    ‚îî‚îÄ NSG Association with Private Subnet
```

#### Module: VM (modules/vm/)
```
21. azurerm_public_ip.vm
    ‚îî‚îÄ VM Public IP
22. azurerm_network_interface.main
    ‚îî‚îÄ Network Interface
23. azurerm_network_interface_security_group_association.main
    ‚îî‚îÄ NIC-NSG Association
24. azurerm_linux_virtual_machine.main
    ‚îî‚îÄ Linux VM: testcontainers-{env}-vm
25. azurerm_key_vault.main
    ‚îî‚îÄ Key Vault: testcontainers-{env}-kv-{random}
26. azurerm_key_vault_access_policy.terraform
    ‚îî‚îÄ Key Vault Access Policy
27. azurerm_key_vault_secret.ssh_private_key
    ‚îî‚îÄ SSH Private Key Secret
```

### Resources NOT Created by Terraform

These are created by setup scripts or manually:

```
28. azurerm_resource_group (Terraform state backend)
    ‚îî‚îÄ testcontainers-tfstate-rg
29. azurerm_storage_account (Terraform state storage)
    ‚îî‚îÄ testcontainerstfstate{subscription_short}
30. azurerm_storage_container (Terraform state container)
    ‚îî‚îÄ tfstate
```

**Total Resources:**
- **Infrastructure Resources:** 27 (created by Terraform)
- **Backend Resources:** 3 (created by setup script, preserved)

---

## 2. Rollback Workflow Analysis

### Workflow: `deploy-azure-infrastructure.yml`

#### Job: `rollback-on-failure`

**Trigger Condition:**
```yaml
needs: [setup-backend, generate-runner-token, terraform-plan, terraform-apply]
if: failure()
```
‚úÖ **Correct:** Runs only when deployment fails

---

### Step-by-Step Rollback Process

#### Step 1: Check State File Existence
```yaml
- name: Check if infrastructure was partially deployed
  id: check-state
  run: |
    BLOB_EXISTS=$(az storage blob exists \
      --account-name ${{ needs.setup-backend.outputs.backend_storage_account }} \
      --container-name tfstate \
      --name azure/${{ env.ENVIRONMENT }}/${{ env.ENVIRONMENT_TAG }}/terraform.tfstate \
      --auth-mode login \
      --query exists -o tsv)
    echo "state_exists=$BLOB_EXISTS" >> $GITHUB_OUTPUT
```

‚úÖ **Correct:** Checks if Terraform state exists before attempting destroy  
‚úÖ **Safe:** Uses `continue-on-error: true` to avoid workflow failure if storage account doesn't exist

---

#### Step 2: Destroy Partially Created Infrastructure
```yaml
- name: Destroy partially created infrastructure
  if: steps.check-state.outputs.state_exists == 'true'
  run: |
    terraform init
    
    # Destroy all modules EXCEPT OIDC
    terraform destroy \
      -target=module.vm \
      -target=module.security \
      -target=module.networking \
      -target=azurerm_resource_group.main \
      -auto-approve
```

**Resources Targeted:**
- ‚úÖ `module.vm` ‚Üí Destroys VM, NIC, Public IP, **Key Vault**, Key Vault secrets
- ‚úÖ `module.security` ‚Üí Destroys NSG, NSG rules, NSG associations
- ‚úÖ `module.networking` ‚Üí Destroys VNet, Subnets, NAT Gateway, NAT Public IP
- ‚úÖ `azurerm_resource_group.main` ‚Üí Destroys resource group

**Resources Preserved:**
- ‚úÖ `module.oidc` ‚Üí Preserved (intentionally not targeted)
  - Azure AD Application
  - Federated Credentials
  - Service Principal
  - Role Assignments

**Analysis:**
‚úÖ **Correct Behavior:** All infrastructure resources are destroyed  
‚úÖ **OIDC Preservation:** OIDC is shared across environments, correctly preserved  
‚ö†Ô∏è **Potential Issue:** Key Vault has soft-delete enabled by default

---

#### Step 3: Clean Up Terraform State
```yaml
- name: Clean up Terraform state
  if: always()
  run: |
    az storage blob delete \
      --account-name ${{ needs.setup-backend.outputs.backend_storage_account }} \
      --container-name tfstate \
      --name azure/${{ env.ENVIRONMENT }}/${{ env.ENVIRONMENT_TAG }}/terraform.tfstate \
      --auth-mode login || true
```

‚úÖ **Correct:** Removes state file after destroy  
‚úÖ **Safe:** Uses `|| true` to avoid failure if blob doesn't exist  
‚úÖ **Complete:** Runs with `if: always()` to ensure cleanup even if destroy fails

---

### Rollback Coverage Matrix

| Resource Type | Targeted for Destroy? | Notes |
|---------------|----------------------|-------|
| **VM Module** | ‚úÖ Yes | Includes VM, NIC, Public IP, Key Vault |
| ‚Ä¢ VM | ‚úÖ Yes (implicit) | Part of module.vm |
| ‚Ä¢ NIC | ‚úÖ Yes (implicit) | Part of module.vm |
| ‚Ä¢ VM Public IP | ‚úÖ Yes (implicit) | Part of module.vm |
| ‚Ä¢ NIC-NSG Association | ‚úÖ Yes (implicit) | Part of module.vm |
| ‚Ä¢ Key Vault | ‚úÖ Yes (implicit) | Part of module.vm |
| ‚Ä¢ Key Vault Access Policy | ‚úÖ Yes (implicit) | Part of module.vm |
| ‚Ä¢ Key Vault Secret | ‚úÖ Yes (implicit) | Part of module.vm |
| **Security Module** | ‚úÖ Yes | Includes NSG, rules, associations |
| ‚Ä¢ NSG | ‚úÖ Yes (implicit) | Part of module.security |
| ‚Ä¢ NSG Rules (3) | ‚úÖ Yes (implicit) | Part of module.security |
| ‚Ä¢ NSG-Subnet Association | ‚úÖ Yes (implicit) | Part of module.security |
| **Networking Module** | ‚úÖ Yes | Includes VNet, subnets, NAT |
| ‚Ä¢ VNet | ‚úÖ Yes (implicit) | Part of module.networking |
| ‚Ä¢ Public Subnet | ‚úÖ Yes (implicit) | Part of module.networking |
| ‚Ä¢ Private Subnet | ‚úÖ Yes (implicit) | Part of module.networking |
| ‚Ä¢ NAT Gateway Public IP | ‚úÖ Yes (implicit) | Part of module.networking |
| ‚Ä¢ NAT Gateway | ‚úÖ Yes (implicit) | Part of module.networking |
| ‚Ä¢ NAT-IP Association | ‚úÖ Yes (implicit) | Part of module.networking |
| ‚Ä¢ Subnet-NAT Association | ‚úÖ Yes (implicit) | Part of module.networking |
| **Resource Group** | ‚úÖ Yes | Explicitly targeted |
| **OIDC Module** | ‚ùå No (intentional) | Shared across environments |
| ‚Ä¢ Azure AD App | ‚ùå No (intentional) | Reused for all deployments |
| ‚Ä¢ Federated Credentials | ‚ùå No (intentional) | Reused for all deployments |
| ‚Ä¢ Service Principal | ‚ùå No (intentional) | Reused for all deployments |
| ‚Ä¢ Role Assignments | ‚ùå No (intentional) | Reused for all deployments |
| **State File** | ‚úÖ Yes | Explicitly deleted |

**Coverage:** 23/27 resources destroyed (85%)  
**Intentionally Preserved:** 4 OIDC resources (shared infrastructure)

---

## 3. Destroy Workflow Analysis

### Workflow: `destroy-azure-infrastructure.yml`

#### Job: `validate-destroy`

```yaml
- name: Check confirmation
  run: |
    if [ "${{ github.event.inputs.confirm_destroy }}" != "destroy" ]; then
      echo "‚ùå Destroy confirmation failed. You must type 'destroy' to proceed."
      exit 1
    fi
```

‚úÖ **Safety Check:** Requires user to type "destroy" to prevent accidental deletion

---

#### Job: `terraform-destroy`

**Destroy Command:**
```yaml
- name: Terraform Destroy
  run: |
    # Destroy all modules EXCEPT OIDC to preserve authentication
    terraform destroy \
      -target=module.vm \
      -target=module.security \
      -target=module.networking \
      -target=azurerm_resource_group.main \
      -auto-approve \
      -input=false
```

**Resources Targeted:**
- ‚úÖ `module.vm` ‚Üí Destroys VM, NIC, Public IP, Key Vault
- ‚úÖ `module.security` ‚Üí Destroys NSG and rules
- ‚úÖ `module.networking` ‚Üí Destroys VNet, Subnets, NAT Gateway
- ‚úÖ `azurerm_resource_group.main` ‚Üí Destroys resource group

**Resources Preserved:**
- ‚úÖ `module.oidc` ‚Üí Intentionally preserved (shared)
- ‚úÖ Backend storage account ‚Üí Intentionally preserved
- ‚úÖ Backend resource group ‚Üí Intentionally preserved

---

### Destroy Coverage Matrix

**Identical to Rollback Coverage:**

| Resource Type | Destroyed? | Preserved? | Reason |
|---------------|-----------|------------|---------|
| VM + dependencies | ‚úÖ Yes | ‚ùå | Part of module.vm |
| Key Vault + secrets | ‚úÖ Yes | ‚ùå | Part of module.vm |
| NSG + rules | ‚úÖ Yes | ‚ùå | Part of module.security |
| VNet + NAT | ‚úÖ Yes | ‚ùå | Part of module.networking |
| Resource Group | ‚úÖ Yes | ‚ùå | Explicitly targeted |
| OIDC (4 resources) | ‚ùå No | ‚úÖ Yes | Shared across environments |
| Backend (3 resources) | ‚ùå No | ‚úÖ Yes | Shared Terraform state |

**Coverage:** 23/27 infrastructure resources destroyed (100% of intended targets)  
**Intentionally Preserved:** 7 shared resources (OIDC + backend)

---

## 4. Critical Findings

### ‚úÖ What Works Correctly

1. **Rollback Completeness:**
   - All infrastructure resources are properly destroyed on failure
   - State file is cleaned up after rollback
   - Workflow uses `continue-on-error` to prevent cascading failures

2. **Destroy Completeness:**
   - All infrastructure resources are properly destroyed
   - Safety confirmation prevents accidental deletion
   - Environment protection requires approval

3. **OIDC Preservation:**
   - Both workflows correctly preserve OIDC configuration
   - OIDC is shared across all environments
   - Prevents need to reconfigure authentication

4. **Backend Preservation:**
   - Backend storage account is preserved
   - Allows future deployments to same environment
   - State history is maintained

5. **Module-Based Destruction:**
   - Using `-target=module.xxx` ensures all resources in module are destroyed
   - Handles implicit dependencies correctly
   - Resource order handled by Terraform automatically

---

### ‚ö†Ô∏è Potential Issues

#### Issue 1: Key Vault Soft Delete

**Problem:**
Azure Key Vaults have soft-delete enabled by default (90-day retention). When destroyed, Key Vault enters "soft-deleted" state rather than being fully deleted.

**Impact:**
- Redeploying with same environment tag may fail: "Key Vault name already exists"
- Key Vault remains in subscription (though not visible in portal by default)
- Costs ~$0.03/month in soft-deleted state

**Current Behavior:**
```hcl
# modules/vm/main.tf
resource "azurerm_key_vault" "main" {
  name                = "${var.project_name}-${var.environment}-kv-${random_string.kv_suffix.result}"
  # soft_delete_retention_days defaults to 90
  # purge_protection_enabled defaults to false
}
```

**Recommendation:**
Either:
1. Add purge operation after destroy
2. Use deterministic naming with recovery
3. Reduce soft-delete retention to minimum (7 days)

**Solution A: Purge After Destroy (Recommended)**
```yaml
# Add to rollback-on-failure and terraform-destroy jobs
- name: Purge deleted Key Vaults
  if: always()
  continue-on-error: true
  run: |
    # List soft-deleted key vaults for this environment
    DELETED_KVS=$(az keyvault list-deleted \
      --query "[?tags.Environment=='${{ env.ENVIRONMENT }}' && tags.EnvironmentTag=='${{ env.ENVIRONMENT_TAG }}'].name" \
      -o tsv)
    
    # Purge each soft-deleted key vault
    for KV_NAME in $DELETED_KVS; do
      echo "Purging Key Vault: $KV_NAME"
      az keyvault purge --name $KV_NAME || true
    done
```

**Solution B: Enable Key Vault Recovery (Alternative)**
```hcl
# modules/vm/main.tf
resource "azurerm_key_vault" "main" {
  name                        = "${var.project_name}-${var.environment}-kv-${random_string.kv_suffix.result}"
  soft_delete_retention_days  = 7  # Minimum retention
  enable_rbac_authorization   = true
  
  lifecycle {
    # Allow Terraform to recover soft-deleted Key Vault
    ignore_changes = [
      soft_delete_retention_days
    ]
  }
}
```

---

#### Issue 2: NAT Gateway Costs During Stopped State

**Problem:**
NAT Gateway continues running 24/7 even when VM is stopped, incurring charges ($42.48/month).

**Impact:**
- High costs when infrastructure is not in use
- Negates savings from stopping VM

**Current Behavior:**
```hcl
# modules/networking/main.tf
resource "azurerm_nat_gateway" "main" {
  name                = "${var.project_name}-${var.environment}-nat"
  # Always running - no stop/start capability
}
```

**Recommendation:**
Document cost implications and provide script to destroy/recreate NAT Gateway:

```bash
#!/bin/bash
# scripts/azure-stop-infrastructure.sh

echo "Stopping Azure VM and removing NAT Gateway to minimize costs..."

# Stop (deallocate) VM
az vm deallocate --resource-group testcontainers-dev-rg --name testcontainers-dev-vm

# Destroy NAT Gateway using Terraform
cd Azure/terraform
terraform destroy \
  -target=module.networking.azurerm_nat_gateway.main \
  -target=module.networking.azurerm_nat_gateway_public_ip_association.main \
  -target=module.networking.azurerm_subnet_nat_gateway_association.main \
  -target=module.networking.azurerm_public_ip.nat \
  -auto-approve

echo "‚úÖ VM deallocated and NAT Gateway removed"
echo "Monthly cost reduced from ~$49 to ~$20"
```

---

#### Issue 3: Resource Dependencies Not Explicitly Managed

**Observation:**
Destroy commands rely on Terraform's implicit dependency resolution.

**Current Behavior:**
```yaml
terraform destroy \
  -target=module.vm \
  -target=module.security \
  -target=module.networking \
  -target=azurerm_resource_group.main \
  -auto-approve
```

**Potential Issue:**
If destroy order matters, Terraform might fail due to dependency conflicts.

**Test Case:**
What happens if:
- Resource Group is destroyed before modules?
- NSG is destroyed before NIC-NSG association?

**Terraform's Behavior:**
‚úÖ Terraform automatically determines correct destroy order based on dependencies  
‚úÖ `-target` includes all dependent resources  
‚úÖ Order of `-target` flags doesn't matter

**Conclusion:**
‚úÖ **No Issue:** Terraform handles this correctly

---

#### Issue 4: Orphaned Resources in Case of Partial Failure

**Scenario:**
1. Terraform starts destroying resources
2. Destroy fails mid-way (e.g., network error)
3. Some resources destroyed, some remain
4. State file is out of sync

**Current Mitigation:**
```yaml
continue-on-error: true
```

**Issue:**
If destroy fails, state cleanup still runs, removing state file. This can orphan resources.

**Example Flow:**
```
1. Start destroy
2. VM destroyed ‚úÖ
3. NIC destroyed ‚úÖ
4. VNet destroy fails ‚ùå (network error)
5. State cleanup runs ‚úÖ (due to always())
6. State file deleted ‚úÖ
7. Result: VNet, NSG, Resource Group orphaned (no state to track them)
```

**Recommendation:**
Only delete state file if destroy succeeds:

```yaml
- name: Destroy partially created infrastructure
  id: destroy
  continue-on-error: true
  run: |
    terraform destroy ... -auto-approve
    echo "success=true" >> $GITHUB_OUTPUT

- name: Clean up Terraform state
  if: steps.destroy.outputs.success == 'true'  # Only if destroy succeeded
  run: |
    az storage blob delete ...
```

---

### ‚úÖ What's Already Correct

1. **Module-Based Targeting:**
   - Using `-target=module.xxx` correctly destroys all resources in module
   - Implicit resources (associations, dependencies) are handled automatically

2. **Error Handling:**
   - `continue-on-error: true` prevents workflow failure on expected errors
   - `|| true` in bash prevents script failure on missing resources

3. **State Management:**
   - State file path is correctly derived: `azure/{env}/{env_tag}/terraform.tfstate`
   - Backend configuration is dynamically generated per environment

4. **OIDC Preservation:**
   - Correctly excludes `module.oidc` from destroy operations
   - OIDC resources are shared across all environments
   - One-time setup, reused for all deployments

5. **Security:**
   - Requires explicit confirmation ("destroy") for manual destroy
   - Uses environment protection for additional approval
   - Uses OIDC for secure authentication (no secrets stored)

---

## 5. Recommendations

### High Priority (Implement Now)

#### 1. Add Key Vault Purge to Workflows

**Why:** Prevent Key Vault name conflicts on redeployment

**Implementation:**
Add to both `rollback-on-failure` and `terraform-destroy` jobs:

```yaml
- name: Purge soft-deleted Key Vaults
  if: always()
  continue-on-error: true
  run: |
    echo "Checking for soft-deleted Key Vaults..."
    
    # List soft-deleted key vaults for this environment
    DELETED_KVS=$(az keyvault list-deleted \
      --query "[?tags.Environment=='${{ env.ENVIRONMENT }}' && tags.EnvironmentTag=='${{ env.ENVIRONMENT_TAG }}'].name" \
      -o tsv 2>/dev/null || echo "")
    
    if [ -z "$DELETED_KVS" ]; then
      echo "No soft-deleted Key Vaults found"
      exit 0
    fi
    
    # Purge each soft-deleted key vault
    for KV_NAME in $DELETED_KVS; do
      echo "Purging Key Vault: $KV_NAME"
      az keyvault purge --name "$KV_NAME" --no-wait || true
    done
    
    echo "‚úÖ Key Vault purge initiated"
```

---

#### 2. Improve State Cleanup Logic

**Why:** Prevent orphaned resources if destroy fails

**Current:**
```yaml
- name: Clean up Terraform state
  if: always()
  run: az storage blob delete ...
```

**Improved:**
```yaml
- name: Destroy partially created infrastructure
  id: destroy
  continue-on-error: true
  run: |
    terraform destroy \
      -target=module.vm \
      -target=module.security \
      -target=module.networking \
      -target=azurerm_resource_group.main \
      -auto-approve
    
    # Capture exit code
    DESTROY_EXIT_CODE=$?
    echo "exit_code=$DESTROY_EXIT_CODE" >> $GITHUB_OUTPUT
    
    if [ $DESTROY_EXIT_CODE -eq 0 ]; then
      echo "success=true" >> $GITHUB_OUTPUT
    else
      echo "success=false" >> $GITHUB_OUTPUT
    fi

- name: Clean up Terraform state
  # Only delete state if destroy was successful
  if: steps.destroy.outputs.success == 'true'
  run: |
    echo "Destroy succeeded, cleaning up state file..."
    az storage blob delete \
      --account-name ${{ needs.setup-backend.outputs.backend_storage_account }} \
      --container-name tfstate \
      --name azure/${{ env.ENVIRONMENT }}/${{ env.ENVIRONMENT_TAG }}/terraform.tfstate \
      --auth-mode login

- name: State cleanup skipped
  if: steps.destroy.outputs.success != 'true'
  run: |
    echo "‚ö†Ô∏è Destroy failed or was skipped, preserving state file for manual cleanup"
    echo "To manually clean up:"
    echo "1. Fix the destroy issue"
    echo "2. Run: terraform destroy"
    echo "3. Delete state file manually if needed"
```

---

### Medium Priority (Consider Implementing)

#### 3. Add Resource Group Lock Removal

**Why:** Prevent destroy failures if resource locks are enabled

```yaml
- name: Remove resource locks before destroy
  continue-on-error: true
  run: |
    RG_NAME="${{ var.project_name }}-${{ env.ENVIRONMENT }}-rg"
    
    # List and delete all locks on resource group
    LOCKS=$(az lock list --resource-group "$RG_NAME" --query "[].id" -o tsv)
    
    for LOCK_ID in $LOCKS; do
      echo "Removing lock: $LOCK_ID"
      az lock delete --ids "$LOCK_ID"
    done
```

---

#### 4. Add Destroy Verification

**Why:** Confirm all resources were actually deleted

```yaml
- name: Verify destruction
  if: always()
  run: |
    RG_NAME="${{ var.project_name }}-${{ env.ENVIRONMENT }}-rg"
    
    # Check if resource group still exists
    if az group exists --name "$RG_NAME"; then
      echo "‚ö†Ô∏è Warning: Resource group still exists"
      echo "Listing remaining resources:"
      az resource list --resource-group "$RG_NAME" --output table
    else
      echo "‚úÖ Resource group successfully deleted"
    fi
    
    # Check for soft-deleted Key Vaults
    DELETED_KVS=$(az keyvault list-deleted \
      --query "[?tags.EnvironmentTag=='${{ env.ENVIRONMENT_TAG }}'].name" \
      -o tsv)
    
    if [ -n "$DELETED_KVS" ]; then
      echo "‚ÑπÔ∏è Soft-deleted Key Vaults (will be purged automatically):"
      echo "$DELETED_KVS"
    fi
```

---

#### 5. Create Cost Optimization Script

**Why:** Allow users to minimize costs when infrastructure is not in use

Create `infrastructure/Azure/scripts/stop-infrastructure.sh`:

```bash
#!/bin/bash
set -e

ENVIRONMENT="${1:-dev}"
ENVIRONMENT_TAG="${2}"

if [ -z "$ENVIRONMENT_TAG" ]; then
  echo "Usage: $0 <environment> <environment-tag>"
  echo "Example: $0 dev SIT-alok-team1-20251124-1400"
  exit 1
fi

RG_NAME="testcontainers-${ENVIRONMENT}-rg"
VM_NAME="testcontainers-${ENVIRONMENT}-vm"

echo "üõë Stopping Azure infrastructure to minimize costs..."
echo "Environment: $ENVIRONMENT"
echo "Environment Tag: $ENVIRONMENT_TAG"
echo ""

# 1. Deallocate VM
echo "1. Deallocating VM (saves compute costs)..."
az vm deallocate --resource-group "$RG_NAME" --name "$VM_NAME"
echo "   ‚úÖ VM deallocated"

# 2. Remove NAT Gateway (saves $42.48/month)
echo "2. Removing NAT Gateway to save costs..."
cd "$(dirname "$0")/../terraform"

terraform destroy \
  -target=module.networking.azurerm_subnet_nat_gateway_association.main \
  -target=module.networking.azurerm_nat_gateway_public_ip_association.main \
  -target=module.networking.azurerm_nat_gateway.main \
  -target=module.networking.azurerm_public_ip.nat \
  -auto-approve

echo "   ‚úÖ NAT Gateway removed"
echo ""
echo "üìä Cost Savings:"
echo "   Before: ~$49/month"
echo "   After:  ~$20/month (60% savings)"
echo ""
echo "‚ÑπÔ∏è  To restart infrastructure:"
echo "   ./scripts/start-infrastructure.sh $ENVIRONMENT $ENVIRONMENT_TAG"
```

Create `infrastructure/Azure/scripts/start-infrastructure.sh`:

```bash
#!/bin/bash
set -e

ENVIRONMENT="${1:-dev}"
ENVIRONMENT_TAG="${2}"

if [ -z "$ENVIRONMENT_TAG" ]; then
  echo "Usage: $0 <environment> <environment-tag>"
  exit 1
fi

RG_NAME="testcontainers-${ENVIRONMENT}-rg"
VM_NAME="testcontainers-${ENVIRONMENT}-vm"

echo "üöÄ Starting Azure infrastructure..."

# 1. Recreate NAT Gateway
echo "1. Recreating NAT Gateway..."
cd "$(dirname "$0")/../terraform"

terraform apply \
  -target=module.networking.azurerm_public_ip.nat \
  -target=module.networking.azurerm_nat_gateway.main \
  -target=module.networking.azurerm_nat_gateway_public_ip_association.main \
  -target=module.networking.azurerm_subnet_nat_gateway_association.main \
  -auto-approve

echo "   ‚úÖ NAT Gateway created"

# 2. Start VM
echo "2. Starting VM..."
az vm start --resource-group "$RG_NAME" --name "$VM_NAME"
echo "   ‚úÖ VM started"
echo ""
echo "‚úÖ Infrastructure is now running"
```

---

### Low Priority (Nice to Have)

#### 6. Add Slack/Teams Notifications

```yaml
- name: Notify on destroy
  if: always()
  run: |
    # Send notification to Slack/Teams
    curl -X POST "${{ secrets.SLACK_WEBHOOK_URL }}" \
      -H 'Content-Type: application/json' \
      -d '{
        "text": "Azure Infrastructure Destroyed",
        "blocks": [{
          "type": "section",
          "text": {
            "type": "mrkdwn",
            "text": "Environment: ${{ env.ENVIRONMENT }}\nTag: ${{ env.ENVIRONMENT_TAG }}\nStatus: Success"
          }
        }]
      }'
```

---

## 6. Testing Checklist

### Rollback Testing

- [ ] Test rollback when VM creation fails
- [ ] Test rollback when network creation fails
- [ ] Test rollback when Key Vault creation fails
- [ ] Verify state file is deleted after rollback
- [ ] Verify OIDC resources are preserved
- [ ] Verify soft-deleted Key Vault is purged (after implementing recommendation)

### Destroy Testing

- [ ] Test manual destroy with confirmation
- [ ] Test destroy rejects without "destroy" confirmation
- [ ] Verify all infrastructure resources are deleted
- [ ] Verify OIDC resources are preserved
- [ ] Verify backend resources are preserved
- [ ] Verify state file remains for audit (or is archived)
- [ ] Check for orphaned resources in Azure Portal

### Edge Cases

- [ ] Test destroy when some resources are already deleted manually
- [ ] Test destroy when Resource Group has locks
- [ ] Test rollback when state file doesn't exist
- [ ] Test destroy when backend storage account doesn't exist
- [ ] Test multiple environments with same OIDC (verify isolation)

---

## 7. Conclusion

### Current State: ‚úÖ Generally Correct

Both workflows properly destroy infrastructure resources with appropriate safeguards:

**Strengths:**
- ‚úÖ Complete coverage of all infrastructure resources
- ‚úÖ Proper OIDC preservation (shared infrastructure)
- ‚úÖ Safe error handling with `continue-on-error`
- ‚úÖ State cleanup after destroy
- ‚úÖ Safety confirmation for manual destroy

**Identified Gaps:**
1. ‚ö†Ô∏è Key Vault soft-delete not handled (minor)
2. ‚ö†Ô∏è State cleanup runs even if destroy fails (could orphan resources)
3. ‚ÑπÔ∏è NAT Gateway cost optimization not documented

**Impact:**
- **Critical:** None (workflows work correctly for intended purpose)
- **High:** State cleanup logic should be improved
- **Medium:** Key Vault purge should be added
- **Low:** Cost optimization scripts would be helpful

### Recommendation: 

**Implement high-priority improvements (#1 and #2) before production use.**

The workflows are functionally correct and will properly destroy infrastructure. The recommended improvements add robustness and prevent edge case issues.

---

## 8. Resource Tagging Strategy

### Current Implementation

All Azure resources are tagged with consistent metadata matching AWS strategy:

```hcl
tags = {
  Environment    = var.environment       # e.g., "dev", "staging", "prod"
  EnvironmentTag = var.environment_tag   # e.g., "SIT-alok-team1-20251124-1400"
  Project        = var.project_name      # e.g., "testcontainers"
  ManagedBy      = "Terraform"          # Infrastructure as Code tracking
}
```

### Benefits

1. **Cost Allocation**: Track costs per environment and environment tag
2. **Resource Discovery**: Find all resources for a specific test environment
3. **Cleanup Verification**: Identify orphaned resources by environment tag
4. **Audit Trail**: Track which resources are managed by Terraform
5. **Multi-Environment Support**: Isolate resources by environment tag

### Tag Usage in Workflows

**In Rollback (WORKFLOW_ANALYSIS.md Issue #1):**
```bash
# Purge soft-deleted Key Vaults for specific environment
az keyvault list-deleted \
  --query "[?tags.EnvironmentTag=='${{ env.ENVIRONMENT_TAG }}'].name"
```

**In Cost Analysis:**
```bash
# Get monthly costs by environment tag
az consumption usage list \
  --query "[?tags.EnvironmentTag=='SIT-alok-team1-20251124-1400']" \
  --output table
```

**In Resource Discovery:**
```bash
# Find all resources for a specific environment
az resource list \
  --tag EnvironmentTag=SIT-alok-team1-20251124-1400 \
  --output table
```

### Comparison with AWS

| Feature | AWS | Azure | Status |
|---------|-----|-------|--------|
| Environment Tag | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Consistent |
| EnvironmentTag (unique) | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Consistent |
| Project Tag | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Consistent |
| ManagedBy Tag | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Consistent |
| Provider-level Tags | ‚úÖ default_tags | ‚ùå Manual | ‚ö†Ô∏è Manual in modules |

**Note:** AWS uses `provider.default_tags` to apply tags automatically, while Azure requires explicit `tags` blocks in each resource.

---

## Quick Reference

### What Gets Destroyed on Rollback/Destroy?

| Resource | Destroyed? | Why? |
|----------|-----------|------|
| Virtual Machine | ‚úÖ Yes | Infrastructure |
| Network Interface | ‚úÖ Yes | Infrastructure |
| VM Public IP | ‚úÖ Yes | Infrastructure |
| Key Vault | ‚úÖ Yes | Infrastructure (soft-deleted) |
| NSG + Rules | ‚úÖ Yes | Infrastructure |
| VNet + Subnets | ‚úÖ Yes | Infrastructure |
| NAT Gateway | ‚úÖ Yes | Infrastructure |
| Resource Group | ‚úÖ Yes | Infrastructure |
| OIDC App | ‚ùå No | Shared (reusable) |
| Service Principal | ‚ùå No | Shared (reusable) |
| Role Assignments | ‚ùå No | Shared (reusable) |
| Backend Storage | ‚ùå No | Shared (Terraform state) |
| State File | ‚úÖ Yes | Cleanup |

### Commands to Verify Cleanup

```bash
# Check if resource group exists
az group exists --name testcontainers-dev-rg

# List resources in resource group
az resource list --resource-group testcontainers-dev-rg --output table

# Check soft-deleted Key Vaults
az keyvault list-deleted --query "[].{Name:name,Location:location,ScheduledPurgeDate:scheduledPurgeDate}"

# Check OIDC app (should still exist)
az ad app list --display-name "testcontainers-dev-github-actions"

# Check backend storage (should still exist)
az storage account list --resource-group testcontainers-tfstate-rg
```
